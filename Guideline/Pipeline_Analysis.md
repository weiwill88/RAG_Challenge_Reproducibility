# RAG 流水线技术分析

本文档深入剖析复现RAG优胜方案过程中的关键技术模块，旨在阐明其工作原理、设计思路和预期效果，作为 `Operational_Diary.md` 操作手册的技术补充。

---

## 阶段1: PDF 解析

### 1. 解析目的 (Purpose)

PDF 解析是整个 RAG 流水线的**起点和基石**。其核心目标是将**非结构化**的 PDF 文档（本质是数字纸张，混杂着文本、图片、表格）转化为大语言模型（LLM）可以理解和处理的**结构化、纯文本数据**。如果这一步的质量不高（例如，文本乱码、表格错乱、段落顺序错误），后续的检索和生成结果将会受到严重影响，正所谓“垃圾进，垃圾出”(Garbage In, Garbage Out)。

### 2. 实现工具与逻辑 (Tools & Logic)

*   **核心工具**: 该项目使用了 `docling` 库。它是一个专门为文档智能（Document Intelligence）设计的强大工具，远超普通的 `PyPDF2` 等库。
*   **核心逻辑**:
    1.  **布局分析 (Layout Analysis)**: `docling` 首先会分析每个页面的视觉布局，识别出不同的内容块，如标题、段落、列表、图片和**表格**。
    2.  **光学字符识别 (OCR)**: 对于 PDF 中的图片或扫描部分，`docling` 会调用其内置的 OCR 引擎（如此前日志中下载的 `easyocr` 模型）来提取其中的文字。
    3.  **文本排序 (Text Ordering)**: 它会根据视觉阅读顺序（从左到右，从上到下）对提取的文本块进行排序，而不是简单地按 PDF 内部存储的混乱顺序提取，这保证了文本的逻辑连贯性。
    4.  **GPU 加速**: 整个过程（特别是模型推理和 OCR）由您的 **RTX 4090 GPU** 提供了强大的算力支持，使得处理复杂文档的速度大大加快。

### 3. 结果文件 (Result Files)

*   **路径**: 解析任务的产出位于 `data/test_set/debug_data/parsed_reports/` 目录中。
*   **文件**: 里面应该有与您保留的 PDF 数量相对应的 `.json` 文件。
*   **内容**: 每个 JSON 文件都是一个结构化的数据体，完整记录了一份报告的所有信息。其内部大致结构为：
    *   `metainfo`: 包含公司名称、报告年份等元数据。
    *   `content`:
        *   `pages`: 一个列表，每个元素代表一页，包含了该页的文本块、表格等所有内容。
        *   `chunks`: 对所有文本内容按照一定规则（如 300 token）进行切分后得到的文本块列表，这是后续构建向量数据库的基础。

### 4. 预期效果 (Expected Effect)

您现在得到的 JSON 文件，是高质量的、干净的数据源。具体来说：
*   **文本保真度高**: 文本内容与原文高度一致，没有乱码。
*   **结构保留完好**: 最重要的是，**表格**被准确地识别并抽离出来，段落、列表等结构也得以保留。这使得后续可以进行更精细化的信息检索（比如，只在表格内检索特定数据）。
*   **上下文准确**: 文本块（chunks）与它们所在的原始页码（parent page）相关联，为后续实现“父页面检索”策略打下了基础。

总而言之，我们已经成功地将原始、复杂的 PDF 转换为了后续 RAG 流程所需的高质量“弹药”。 